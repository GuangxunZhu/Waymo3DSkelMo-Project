<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <!-- Primary Meta Tags -->
  <!-- TODO: Replace with your paper title and author names -->
  <meta name="title" content="Waymo-3DSkelMo: A Multi-Agent 3D Skeletal Motion Dataset for Pedestrian Interaction Modeling in Autonomous Driving - Guangxun Zhu, Shiyu Fan, Hang Dai, Edmond S. L. Ho">
  <!-- TODO: Write a compelling 150-160 character description of your research -->
  <meta name="description" content="Waymo-3DSkelMo is a large-scale dataset of high-quality, temporally coherent multi-person 3D skeletal motions with explicit interaction semantics, derived from the [Waymo Perception dataset](https://waymo.com/open/data/perception/) and designed for fine-grained pedestrian interaction understanding in autonomous driving scenarios.">
  <!-- TODO: Add 5-10 relevant keywords for your research area -->
  <meta name="keywords" content="Multi-person interaction, Pedestrian interaction, 3D Skeletal Motion, LiDRA, Motion dataset, Autonomous driving, Machine learning, AI">
  <!-- TODO: List all authors -->
  <meta name="author" content="Guangxun Zhu, Shiyu Fan, Hang Dai, Edmond S. L. Ho">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">
  
  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <!-- TODO: Replace with your institution or lab name -->
  <meta property="og:site_name" content="University of Glasgow, Wuhan University">
  <!-- TODO: Same as paper title above -->
  <meta property="og:title" content="Waymo-3DSkelMo: A Multi-Agent 3D Skeletal Motion Dataset for Pedestrian Interaction Modeling in Autonomous Driving - Guangxun Zhu, Shiyu Fan, Hang Dai, Edmond S. L. Ho">
  <!-- TODO: Same as description above -->
  <meta property="og:description" content="Waymo-3DSkelMo is a large-scale dataset of high-quality, temporally coherent multi-person 3D skeletal motions with explicit interaction semantics, derived from the [Waymo Perception dataset](https://waymo.com/open/data/perception/) and designed for fine-grained pedestrian interaction understanding in autonomous driving scenarios.">
  <!-- TODO: Replace with your actual website URL -->
  <meta property="og:url" content="https://guangxunzhu.github.io/Waymo3DSkelMo-Project/">
  <!-- TODO: Create a 1200x630px preview image and update path -->
  <!-- <meta property="og:image" content="https://YOUR_DOMAIN.com/static/images/social_preview.png">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <meta property="og:image:alt" content="PAPER_TITLE - Research Preview"> -->
  <meta property="article:published_time" content="2025-10-27">
  <meta property="article:author" content="Guangxun Zhu">
  <meta property="article:section" content="Research">
  <meta property="article:tag" content="3D Skeletal Motion">
  <meta property="article:tag" content="LiDAR">
  <meta property="article:tag" content="Autonomous Driving">

  <!-- Twitter -->
  <meta name="twitter:card" content="summary_large_image">
  <!-- TODO: Replace with your lab/institution Twitter handle -->
  <meta name="twitter:site" content="@YOUR_TWITTER_HANDLE">
  <!-- TODO: Replace with first author's Twitter handle -->
  <meta name="twitter:creator" content="@AUTHOR_TWITTER_HANDLE">
  <!-- TODO: Same as paper title above -->
  <meta name="twitter:title" content="Waymo-3DSkelMo: A Multi-Agent 3D Skeletal Motion Dataset for Pedestrian Interaction Modeling in Autonomous Driving">
  <!-- TODO: Same as description above -->
  <meta name="twitter:description" content="Waymo-3DSkelMo is a large-scale dataset of high-quality, temporally coherent multi-person 3D skeletal motions with explicit interaction semantics, derived from the [Waymo Perception dataset](https://waymo.com/open/data/perception/) and designed for fine-grained pedestrian interaction understanding in autonomous driving scenarios.">
  <!-- TODO: Same as social preview image above -->
  <meta name="twitter:image" content="https://YOUR_DOMAIN.com/static/images/social_preview.png">
  <meta name="twitter:image:alt" content="PAPER_TITLE - Research Preview">

  <!-- Academic/Research Specific -->
  <meta name="citation_title" content="Waymo-3DSkelMo: A Multi-Agent 3D Skeletal Motion Dataset for Pedestrian Interaction Modeling in Autonomous Driving">
  <meta name="citation_author" content="Zhu, Guangxun">
  <meta name="citation_author" content="Fan, Shiyu">
  <meta name="citation_author" content="Dai, Hang">
  <meta name="citation_author" content="Ho, Edmond S. L.">
  <meta name="citation_publication_date" content="2025">
  <meta name="citation_conference_title" content="Proceedings of the 33rd ACM International Conference on Multimedia">
  <meta name="citation_pdf_url" content="https://arxiv.org/abs/2508.09404">
  
  <!-- Additional SEO -->
  <meta name="theme-color" content="#2563eb">
  <meta name="msapplication-TileColor" content="#2563eb">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="default">
  
  <!-- Preconnect for performance -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://documentcloud.adobe.com">
  <link rel="preconnect" href="https://cdn.jsdelivr.net">


  <!-- TODO: Replace with your paper title and authors -->
  <title>Waymo-3DSkelMo: A Multi-Agent 3D Skeletal Motion Dataset for Pedestrian Interaction Modeling in Autonomous Driving - Guangxun Zhu, Shiyu Fan, Hang Dai, Edmond S. L. Ho | Academic Research</title>
  
  <!-- Favicon and App Icons -->
  <!-- <link rel="icon" type="image/x-icon" href="static/images/favicon.ico"> -->
  <!-- <link rel="apple-touch-icon" href="static/images/favicon.ico"> -->
  
  <!-- Critical CSS - Load synchronously -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  
  <!-- Non-critical CSS - Load asynchronously -->
  <link rel="preload" href="static/css/bulma-carousel.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  
  <!-- Fallback for browsers that don't support preload -->
  <noscript>
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>
  
  <!-- Fonts - Optimized loading -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
  
  <!-- Defer non-critical JavaScript -->
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>
  
  <!-- Structured Data for Academic Papers -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "Waymo-3DSkelMo: A Multi-Agent 3D Skeletal Motion Dataset for Pedestrian Interaction Modeling in Autonomous Driving",
    "description": "Waymo-3DSkelMo is the first large-scale dataset providing high-quality, temporally coherent 3D skeletal motions with explicit interaction semantics, derived from the Waymo Perception dataset, and includes 3D pose forecasting benchmarks.",
    "author": [
      {
        "@type": "Person",
        "name": "Guangxun Zhu",
        "affiliation": { "@type": "Organization", "name": "University of Glasgow" }
      },
      {
        "@type": "Person",
        "name": "Shiyu Fan",
        "affiliation": { "@type": "Organization", "name": "University of Glasgow" }
      },
      {
        "@type": "Person",
        "name": "Hang Dai",
        "affiliation": [
          { "@type": "Organization", "name": "Wuhan University" },
          { "@type": "Organization", "name": "University of Glasgow" }
        ]
      },
      {
        "@type": "Person",
        "name": "Edmond S. L. Ho",
        "affiliation": { "@type": "Organization", "name": "University of Glasgow" }
      }
    ],
    "datePublished": "2025-10-27",
    "publisher": {
      "@type": "Organization",
      "name": "ACM Multimedia 2025 (Dataset Track)"
    },
    "url": "https://arxiv.org/abs/2508.09404",
    "image": "static/images/social_preview.png",
    "keywords": [
      "Multi-person interaction",
      "Pedestrian interaction",
      "3D skeletal motion",
      "LiDAR",
      "Motion dataset",
      "Autonomous driving",
      "Pose forecasting"
    ],
    "abstract": "Large-scale high-quality 3D motion datasets with multi-person interactions are crucial for data-driven models in autonomous driving to achieve fine-grained pedestrian interaction understanding in dynamic urban environments. However, existing datasets mostly rely on estimating 3D poses from monocular RGB video frames, which suffer from occlusion and lack of temporal continuity, thus resulting in unrealistic and low-quality human motion. In this paper, we introduce Waymo-3DSkelMo, the first large-scale dataset providing high-quality, temporally coherent 3D skeletal motions with explicit interaction semantics, derived from the Waymo Perception dataset. Our key insight is to utilize 3D human body shape and motion priors to enhance the quality of the 3D pose sequences extracted from the raw LiDAR point clouds. The dataset covers over 14,000 seconds across more than 800 real driving scenarios, including rich interactions among an average of 27 agents per scene (with up to 250 agents in the largest scene). Furthermore, we establish 3D pose forecasting benchmarks under varying pedestrian densities, and the results demonstrate its value as a foundational resource for future research on fine-grained human behavior understanding in complex urban environments.",
    "isAccessibleForFree": true,
    "license": "https://creativecommons.org/licenses/by-nc-sa/4.0/",
    "sameAs": [
      "https://arxiv.org/abs/2508.09404",
      "https://github.com/GuangxunZhu/Waymo-3DSkelMo"
    ],
    "mainEntity": { "@type": "WebPage", "@id": "https://arxiv.org/abs/2508.09404" }
  }
  </script>
  
  <!-- Website/Organization Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Organization",
    "name": "University of Glasgow",
    "url": "https://www.gla.ac.uk/",
    "logo": "static/images/favicon.ico"
  }
  </script>
  
</head>
<body>


  <!-- Scroll to Top Button -->
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>

  <!-- More Works Dropdown -->
  <div class="more-works-container">
    <button class="more-works-btn" onclick="toggleMoreWorks()" title="View More Works from Our Lab">
      <i class="fas fa-flask"></i>
      More Works
      <i class="fas fa-chevron-down dropdown-arrow"></i>
    </button>
    <div class="more-works-dropdown" id="moreWorksDropdown">
      <div class="dropdown-header">
        <h4>More Works from Our Lab</h4>
        <button class="close-btn" onclick="toggleMoreWorks()">
          <i class="fas fa-times"></i>
        </button>
      </div>

      <!-- Hidden -->
      <div class="works-list", hidden>
        <!-- TODO: Replace with your lab's related works -->

        <a href="https://arxiv.org/abs/PAPER_ID_1" class="work-item" target="_blank">
          <div class="work-info">
            <!-- TODO: Replace with actual paper title -->
            <h5>Paper Title 1</h5>
            <!-- TODO: Replace with brief description -->
            <p>Brief description of the work and its main contribution.</p>
            <!-- TODO: Replace with venue and year -->
            <span class="work-venue">Conference/Journal 2024</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>
        <!-- TODO: Add more related works or remove extra items -->
        <a href="https://arxiv.org/abs/PAPER_ID_2" class="work-item" target="_blank">
          <div class="work-info">
            <h5>Paper Title 2</h5>
            <p>Brief description of the work and its main contribution.</p>
            <span class="work-venue">Conference/Journal 2023</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>
        <a href="https://arxiv.org/abs/PAPER_ID_3" class="work-item" target="_blank">
          <div class="work-info">
            <h5>Paper Title 3</h5>
            <p>Brief description of the work and its main contribution.</p>
            <span class="work-venue">Conference/Journal 2023</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>

      </div>
    </div>
  </div>

  <main id="main-content">
  <section class="hero hero-header">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <!-- TODO: Replace with your paper title -->
            <h1 class="title is-2 publication-title">Waymo-3DSkelMo: A Multi-Agent 3D Skeletal Motion Dataset for Pedestrian Interaction Modeling in Autonomous Driving </h1>
            <div class="is-size-5 publication-authors">
              <!-- TODO: Replace with your paper authors and their personal links -->
              <span class="author-block">
                <a href="https://guangxunzhu.github.io" target="_blank">Guangxun Zhu</a><sup>1</sup>,</span>
                <span class="author-block">
                  <a href="https://www.gla.ac.uk/pgrs/shiyufan/" target="_blank">Shiyu Fan</a><sup>1</sup>,</span>
                  <span class="author-block">
                    <a href="https://dai-hang.github.io" target="_blank">Hang Dai</a><sup>1,2</sup>,</span>
                      <span class="author-block">
                        <a href="http://www.edho.net" target="_blank">Edmond S. L. Ho</a><sup>1*</sup>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <!-- TODO: Replace with your institution and conference/journal info -->
                    <span class="author-block"><sup>1</sup> University of Glasgow, <sup>2</sup> Wuhan University<br>ACM Multimedia 2025</span>
                    <!-- TODO: Remove this line if no equal contribution -->
                    <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Corresponding Author</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- TODO: Update with your arXiv paper ID -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2508.09404" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- TODO: Add your supplementary material PDF or remove this section -->
                    <span class="link-block", hidden>
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span>

                  <!-- TODO: Replace with your GitHub repository URL -->
                  <span class="link-block">
                    <a href="https://github.com/GuangxunZhu/Waymo-3DSkelMo" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- TODO: Update with your arXiv paper ID -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2508.09404" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Youtube video -->
<section class="hero is-small hero-after-header">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <!-- Paper video. -->
      <!-- <h2 class="title is-4 has-text-centered">Video Presentation</h2> -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video">
            <!-- TODO: Replace with your YouTube video ID -->
            <iframe src="https://www.youtube.com/embed/HpEj48uxiW4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End youtube video -->

<!-- Teaser video-->
<!-- <section class="hero teaser"> -->
  <!-- <div class="container is-max-desktop"> -->
    <!-- <div class="hero-body"> -->
      <!-- TODO: Replace with your teaser video -->
      <!-- <video poster="" id="tree" autoplay controls muted loop height="100%" preload="metadata"> -->
        <!-- TODO: Add your video file path here -->
        <!-- <source src="static/videos/banner_video.mp4" type="video/mp4"> -->
      <!-- </video> -->
      <!-- TODO: Replace with your video description -->
      <!-- <h2 class="subtitle has-text-centered"> -->
        <!-- Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus.  -->
      <!-- </h2> -->
    <!-- </div> -->
  <!-- </div> -->
<!-- </section> -->
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="hero hero-abstract">
  <div class="hero-body">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <!-- TODO: Replace with your paper abstract -->
          <p>
            Large-scale high-quality 3D motion datasets with multi-person interactions are crucial for data-driven models in autonomous driving to achieve fine-grained pedestrian interaction understanding in dynamic urban environments. However, existing datasets mostly rely on estimating 3D poses from monocular RGB video frames, which suffer from occlusion and lack of temporal continuity, thus resulting in unrealistic and low-quality human motion. In this paper, we introduce Waymo-3DSkelMo, the first large-scale dataset providing high-quality, temporally coherent 3D skeletal motions with explicit interaction semantics, derived from the Waymo Perception dataset. Our key insight is to utilize 3D human body shape and motion priors to enhance the quality of the 3D pose sequences extracted from the raw LiDAR point clouds.
            The dataset covers over 14,000 seconds across more than 800 real driving scenarios, including rich interactions among an average of 27 agents per scene (with up to 250 agents in the largest scene). Furthermore, we establish 3D pose forecasting benchmarks under varying pedestrian densities, and the results demonstrate its value as a foundational resource for future research on fine-grained human behavior understanding in complex urban environments.
          </p>
        </div>
      </div>
    </div>
  </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Image carousel -->
<section class="hero is-small hero-image-caurousel">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- TODO: Replace with your research result images -->
        <img src="static/images/dataset.png" alt="First research result visualization" loading="lazy"/>
        <!-- TODO: Replace with description of this result -->
        <h2 class="subtitle">
          Waymo-3DSkelMo: A high-quality 3D Multi-pedestrian motion dataset created using human motion and shape priors from LiDAR range images in the Waymo perception dataset. (Blue) The point clouds, sampled every 0.5 seconds, of a pedestrian from the LiDAR range images. A 3D body mesh can be estimated from the partial LiDAR point cloud using a 3D human shape prior for each sample. (Purple) The Waymo dataset comes with very sparsely annotated 3D skeletal poses. (Yellow) Based on the skeletal poses extracted from the estimated body meshes, a motion prior is used to enhance the motion quality.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/pipeline.png" alt="Second research result visualization" loading="lazy"/>
        <h2 class="subtitle">
        Overview of our pipeline. 
          (a) Point clouds are first extracted from the range images of all five Waymo LiDAR sensors, then transformed into a world coordinate system and fused into a unified point cloud representation. 
          (b) Mesh recovery is performed on all point clouds using a human-body prior, followed by motion generation via a motion prior.
          (c) Regressing SMPL parameters to skeletal motions. 
          (d) An example of different quality of point cloud and 3D pose.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/examples.png" alt="Third research result visualization" loading="lazy"/>
        <h2 class="subtitle has-text-centered">
         Example scenes from the Waymo-3DSkelMo dataset.
       </h2>
     </div>
     
  </div>
</div>
</div>
</section>
<!-- End image carousel -->

<section class="hero is-small hero-results">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Statistics</h2>
    Comparison of statistics between the newly proposed Waymo-3DSkelMo and existing human pose forecasting datasets.
    <picture>
        <img src="static/images/statistics.png" alt="teaser" />
    </picture>
    <br><br>
    <h2 class="title is-3 has-text-centered">Experiments</h2>
    Quantitative comparison of motion generation methods with and without Frenet-frame alignment. Metrics marked with ↓indicate that lower values are better. Within each setting (with/without Frenet), the best result for each metric is highlighted in bold.
    <picture>
        <img src="static/images/generation_comparison.png" alt="teaser" />
    </picture>
    <strong>Version 2</strong> enhances the optimization by incorporating Waymo-annotated 3D bounding boxes as strong geometric constraints and upweighting high-quality LiDAR-based pseudo-labels, resulting in significantly improved joint-position accuracy and overall motion quality.
    <br><br>
    <h2 class="title is-3 has-text-centered">Benchmarking</h2>
    <strong>Version 2</strong> results of JPE, APE, and FDE (in mm) under different numbers of persons. We compare short-term predictions using TBIFormer across varying levels of multi-person interaction.

    <picture>
        <img src="static/images/tbiformer_results.png" alt="teaser" />
    </picture>
    <!-- <br><br> -->
    <!-- <p class="has-text-centered">CoMapGS improves geometric consistency in sparse view settings.</p> -->
  </div>
</section>


<!-- Video carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered">Waymo-3DSkelMo Dataset</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <!-- TODO: Add poster image for better preview -->
          <video poster="" id="video1" controls muted loop height="100%" preload="metadata">
            <!-- Your video file here -->
            <source src="static/videos/15266427834976906738_1620_000_1640_000.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <!-- TODO: Add poster image for better preview -->
          <video poster="" id="video2" controls muted loop height="100%" preload="metadata">
            <!-- Your video file here -->
            <source src="static/videos/4114454788208078028_660_000_680_000.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <!-- TODO: Add poster image for better preview -->
          <video poster="" id="video3" controls muted loop height="100%" preload="metadata">
            <!-- Your video file here -->
            <source src="static/videos/11252086830380107152_1540_000_1560_000.mp4" type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End video carousel -->






<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title">Poster</h2>

      TODO: Replace with your poster PDF
      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section> -->
<!--End paper poster -->



<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <div class="bibtex-header">
        <h2 class="title">BibTeX</h2>
        <button class="copy-bibtex-btn" onclick="copyBibTeX()" title="Copy BibTeX to clipboard">
          <i class="fas fa-copy"></i>
          <span class="copy-text">Copy</span>
        </button>
      </div>
      <pre id="bibtex-code"><code>@inproceedings{zhu2025waymo,
  title={Waymo-3DSkelMo: A Multi-Agent 3D Skeletal Motion Dataset for Pedestrian Interaction Modeling in Autonomous Driving},
  author={Zhu, Guangxun and Fan, Shiyu and Dai, Hang and Ho, Edmond SL},
  booktitle={Proceedings of the 33rd ACM International Conference on Multimedia},
  pages={13184--13190},
  year={2025}
  }
</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
